{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ulusalfn/Tutorial-1/blob/main/GENAI_Generating_Text_with_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Text with LSTM Networks\n",
        "\n",
        "In this notebook, you will create character-based models for text generation with LSTM recurrent neural networks.\n",
        "\n",
        "**Note:** LSTM models are slow to train. We will start with a small LSTM network and train it for a short time to get a feel for how they work before creating a slightly more complex model and training it for longer to obtain better results.\n"
      ],
      "metadata": {
        "id": "3CF5IQUgagry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Gutenberg\n",
        "\n",
        "Many of the classical texts are no longer protected under copyright. This means you can download all the text for these books for free and use them to experiment with. [Project Gutenberg](https://www.gutenberg.org/) provides an extensive collection of books that are no longer under copyright.\n",
        "\n",
        "For this notebook, we will download the text for [Alice's Adventures in Wonderland](https://www.gutenberg.org/ebooks/11) by Lewis Carroll.\n",
        "\n",
        "Before we start building our model, we will need to prepare the text of this book so that we can easily work with it. The simplest way to do this is to [download the complete plain text (UTF-8) version of the book](https://www.gutenberg.org/cache/epub/11/pg11.txt) to your local machine. Save this version of the text with the filename `wonderland_raw.txt`.\n",
        "\n",
        "Project Gutenberg adds a header and footer information to each book, which is not part of the original text. Because we only want to train our model on the original text, open the file you have just saved in a text editor and delete the header and footer information.\n",
        "\n",
        "The header text ends with:\n",
        "\n",
        "> `*** START OF THIS PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***`\n",
        "\n",
        "The footer is all of the text after the line containing:\n",
        "\n",
        "> `THE END`\n",
        "\n",
        "You should be left with a text file that has a bit over 3,300 lines of text. Save this new version of the file as `wonderland_text.txt`. You will upload this file to this notebook to be the dataset to train your model.\n",
        "\n",
        "To upload the file, you should be able to open the *Files* panel on the left of this notebook and drag the `wonderland_text.txt` into the panel."
      ],
      "metadata": {
        "id": "hxrL_h6MblCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Small LSTM Network\n",
        "\n",
        "In this section, we will build a small LSTM network and train it to predict character sequences from *Alice in Wonderland*.\n",
        "\n",
        "We start by importing the libraries that we'll use to construct and train our model."
      ],
      "metadata": {
        "id": "7OQdn1dXep3b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsLzlJv9OjKz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we load the text file that we've uploaded into memory and convert all of the text to lowercase to reduce the vocabulary (of characters) that the network must learn."
      ],
      "metadata": {
        "id": "MIA5-PqgfIfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"wonderland_text.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()"
      ],
      "metadata": {
        "id": "4zPhbANCOqUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to prepare the data by converting it into an appropriate form to input into a network. We don't model the characters directly, instead, we convert the characters into integer tokens.\n",
        "\n",
        "We do this by first creating a set of all of the different characters in the book, then creating a map (a `dict` in Python) of each character to a unique integer, by enumerating each character in the set and assigning it an integer equal to its index in the set. The particular value of the integers assigned to the chars is not important for the model."
      ],
      "metadata": {
        "id": "zGr4sbzOfaFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "JrKVBScSP4RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can examine the vocabulary of characters that have been extracted from the book by printing the `chars` set, which is a list of the unique lowercase letters, numbers, punctuation and special characters that appear in the text."
      ],
      "metadata": {
        "id": "Yd5ZzShkgMk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyiZJdLOP7g7",
        "outputId": "6f688bb1-70de-4177-f488-5b11f2b641b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '0', '3', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vocabulary still includes some characters that we might want to remove from the text, e.g., `*`, and this may improve the model by removing text we do not want to generate. But this will be sufficient for our purposes of working with an LSTM to generate some text.\n",
        "\n",
        "We can summarise the dataset in terms of the size of the whole text and the size of the vocabulary."
      ],
      "metadata": {
        "id": "b0aNNWPAhEwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: {}\".format(n_chars))\n",
        "print(\"Total Vocab: {}\".format(n_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSRfE3-jQHji",
        "outputId": "ea6198f8-340a-410e-c1a9-1ca13adf4245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters: 144431\n",
            "Total Vocab: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the text has a little under 145,000 characters in total and the vocabulary has around 46 characters. (Your values should be around these values, but you may have a slightly bigger or smaller vocabulary depending on how much you removed from the original raw file.)\n",
        "\n",
        "We now need to define the training data for the network. There is a lot of flexibility in how you choose to break up the text and present it to the network.\n",
        "\n",
        "In this tutorial, we will split the text into sequences with a fixed length of 100 characters. There is nothing particularly special about this length of sequence, it is an arbitrary length, but should be sufficient to demonstrate the use of an LSTM to generate text character-by-character.\n",
        "\n",
        "Another common approach to breaking up a dataset like this is to find the longest sentence in the text and use this as the sequence length. The text can then be broken into sentences, padding the shorter sequences such that the input to the network is of a uniform length.\n",
        "\n",
        "Each training pattern for the network comprises 100 time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it, with the exception of the first 100 characters.\n",
        "\n",
        "For example, if the sequence length were just 5, then the first two training patterns would be as follows:\n",
        "\n",
        "> X: `CHAPT` -> y: `E`  \n",
        "> X: `HAPTE` -> y: `R`\n",
        "\n",
        "As we split the book into sequences, we also convert the characters to integers using the map we prepared earlier."
      ],
      "metadata": {
        "id": "k9sIIaKWh2HH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "  seq_in = raw_text[i:i + seq_length]\n",
        "  seq_out = raw_text[i + seq_length]\n",
        "  dataX.append([char_to_int[char] for char in seq_in])\n",
        "  dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: {}\".format(n_patterns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dudn5r4XQtee",
        "outputId": "60ecad1a-da32-4ed9-b31b-d0962f40ab00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns: 144331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the code to this point shows that when we split up the dataset into training data for the network to learn that we have just under 145,000 training patterns.\n",
        "\n",
        "This makes sense because, excluding the first 100 characters, we have one training pattern to predict each of the remaining characters in the text."
      ],
      "metadata": {
        "id": "hfDSXEzxk6m5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have prepared the training data, we need to transform it again for use with Keras.\n",
        "\n",
        "First, we transform the list of input sequences into the form \\[samples, time steps, features\\], which is expected by an LSTM network.\n",
        "\n",
        "Next, we rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network using the sigmoid activation function by default.\n",
        "\n",
        "Finally, we convert the output patterns (single characters converted to integers) into a one-hot encoding. This is so that we can configure the network to predict the probability of each of the different characters in the vocabulary. Each `y` value is converted into a sparse vector with a length of the vocabulary, full of zeros, except with a single 1 in the column for the letter (integer) that the pattern represents.\n",
        "\n",
        "For example, when “n” (integer value 32) is one-hot encoded, it will look something like:\n",
        "\n",
        "> `[0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]`"
      ],
      "metadata": {
        "id": "5bK8-Kq5lYOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalise the inputs to be in the range 0..1\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the outputs\n",
        "y = to_categorical(dataY)"
      ],
      "metadata": {
        "id": "YsGzl4vOQ9PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now define the LSTM model. Here, we define a single hidden LSTM layer with 256 memory units. The network uses dropout with a probability of 20%. The output layer is a Dense layer using the softmax activation function to output a probability prediction (0..1) for each of the different characters.\n",
        "\n",
        "The problem is really a single character classification problem with the number of classes equal to the size of the vocabulary. As such, the model compiled as optimizing the  categorical crossentropy loss, using the ADAM optimisation algorithm."
      ],
      "metadata": {
        "id": "x7MSfvxSnEsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "qBl1-Uy7RDDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no test dataset in this example. We are modelling the entire training dataset to learn the probability of each character in a sequence. It can be useful to define a validation set, as we would normally for such a categorisation task, in particular to avoid overfitting on the given dataset. But for the sake of simplicity, this has been omitted from this example."
      ],
      "metadata": {
        "id": "GDgrguuhnqiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even a small LSTM network can be slow to train, because of this, we can use model checkpointing to record all the network weights to a file each time an improvement in the loss is observed at the end of the epoch. This will mean that we can use the best set of weights (lowest loss) to instantiate our generative model in the next section."
      ],
      "metadata": {
        "id": "GILr-5GVoK4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.keras\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "1h2YfVTRRJE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now fit your model to the data. Here, we use a modest number of 20 epochs and a large batch size of 128 patterns, to ensure that we can get a result within a relatively short period of time."
      ],
      "metadata": {
        "id": "a9NWtR2bonDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qRsZf6tyRM4e",
        "outputId": "0b1a1f0a-7982-4e78-e729-e55a36468571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1128/1128 [==============================] - ETA: 0s - loss: 1.9355\n",
            "Epoch 1: loss improved from 1.96180 to 1.93548, saving model to weights-improvement-01-1.9355.hdf5\n",
            "1128/1128 [==============================] - 18s 15ms/step - loss: 1.9355\n",
            "Epoch 2/20\n",
            "1125/1128 [============================>.] - ETA: 0s - loss: 1.9128\n",
            "Epoch 2: loss improved from 1.93548 to 1.91284, saving model to weights-improvement-02-1.9128.hdf5\n",
            "1128/1128 [==============================] - 17s 15ms/step - loss: 1.9128\n",
            "Epoch 3/20\n",
            "1126/1128 [============================>.] - ETA: 0s - loss: 1.8915\n",
            "Epoch 3: loss improved from 1.91284 to 1.89150, saving model to weights-improvement-03-1.8915.hdf5\n",
            "1128/1128 [==============================] - 16s 14ms/step - loss: 1.8915\n",
            "Epoch 4/20\n",
            "1128/1128 [==============================] - ETA: 0s - loss: 1.8746\n",
            "Epoch 4: loss improved from 1.89150 to 1.87459, saving model to weights-improvement-04-1.8746.hdf5\n",
            "1128/1128 [==============================] - 16s 14ms/step - loss: 1.8746\n",
            "Epoch 5/20\n",
            "1125/1128 [============================>.] - ETA: 0s - loss: 1.8536\n",
            "Epoch 5: loss improved from 1.87459 to 1.85369, saving model to weights-improvement-05-1.8537.hdf5\n",
            "1128/1128 [==============================] - 16s 14ms/step - loss: 1.8537\n",
            "Epoch 6/20\n",
            "1127/1128 [============================>.] - ETA: 0s - loss: 1.8349\n",
            "Epoch 6: loss improved from 1.85369 to 1.83492, saving model to weights-improvement-06-1.8349.hdf5\n",
            "1128/1128 [==============================] - 16s 14ms/step - loss: 1.8349\n",
            "Epoch 7/20\n",
            "1125/1128 [============================>.] - ETA: 0s - loss: 1.8203\n",
            "Epoch 7: loss improved from 1.83492 to 1.82006, saving model to weights-improvement-07-1.8201.hdf5\n",
            "1128/1128 [==============================] - 17s 15ms/step - loss: 1.8201\n",
            "Epoch 8/20\n",
            "1125/1128 [============================>.] - ETA: 0s - loss: 1.8020\n",
            "Epoch 8: loss improved from 1.82006 to 1.80218, saving model to weights-improvement-08-1.8022.hdf5\n",
            "1128/1128 [==============================] - 17s 15ms/step - loss: 1.8022\n",
            "Epoch 9/20\n",
            "1125/1128 [============================>.] - ETA: 0s - loss: 1.7843\n",
            "Epoch 9: loss improved from 1.80218 to 1.78437, saving model to weights-improvement-09-1.7844.hdf5\n",
            "1128/1128 [==============================] - 16s 14ms/step - loss: 1.7844\n",
            "Epoch 10/20\n",
            "1128/1128 [==============================] - ETA: 0s - loss: 1.8393\n",
            "Epoch 10: loss did not improve from 1.78437\n",
            "1128/1128 [==============================] - 16s 14ms/step - loss: 1.8393\n",
            "Epoch 11/20\n",
            "1126/1128 [============================>.] - ETA: 0s - loss: 1.9350\n",
            "Epoch 11: loss did not improve from 1.78437\n",
            "1128/1128 [==============================] - 16s 14ms/step - loss: 1.9347\n",
            "Epoch 12/20\n",
            "1128/1128 [==============================] - ETA: 0s - loss: 2.3567\n",
            "Epoch 12: loss did not improve from 1.78437\n",
            "1128/1128 [==============================] - 16s 14ms/step - loss: 2.3567\n",
            "Epoch 13/20\n",
            "1127/1128 [============================>.] - ETA: 0s - loss: 2.4263\n",
            "Epoch 13: loss did not improve from 1.78437\n",
            "1128/1128 [==============================] - 16s 15ms/step - loss: 2.4263\n",
            "Epoch 14/20\n",
            "1126/1128 [============================>.] - ETA: 0s - loss: 2.0929\n",
            "Epoch 14: loss did not improve from 1.78437\n",
            "1128/1128 [==============================] - 16s 14ms/step - loss: 2.0927\n",
            "Epoch 15/20\n",
            "1125/1128 [============================>.] - ETA: 0s - loss: 1.9332\n",
            "Epoch 15: loss did not improve from 1.78437\n",
            "1128/1128 [==============================] - 16s 14ms/step - loss: 1.9331\n",
            "Epoch 16/20\n",
            "1127/1128 [============================>.] - ETA: 0s - loss: 1.8365\n",
            "Epoch 16: loss did not improve from 1.78437\n",
            "1128/1128 [==============================] - 17s 15ms/step - loss: 1.8364\n",
            "Epoch 17/20\n",
            "1127/1128 [============================>.] - ETA: 0s - loss: 1.7916\n",
            "Epoch 17: loss did not improve from 1.78437\n",
            "1128/1128 [==============================] - 17s 15ms/step - loss: 1.7915\n",
            "Epoch 18/20\n",
            "1128/1128 [==============================] - ETA: 0s - loss: 1.7731\n",
            "Epoch 18: loss improved from 1.78437 to 1.77310, saving model to weights-improvement-18-1.7731.hdf5\n",
            "1128/1128 [==============================] - 17s 15ms/step - loss: 1.7731\n",
            "Epoch 19/20\n",
            "1127/1128 [============================>.] - ETA: 0s - loss: 1.7578\n",
            "Epoch 19: loss improved from 1.77310 to 1.75796, saving model to weights-improvement-19-1.7580.hdf5\n",
            "1128/1128 [==============================] - 16s 15ms/step - loss: 1.7580\n",
            "Epoch 20/20\n",
            "1128/1128 [==============================] - ETA: 0s - loss: 1.7795\n",
            "Epoch 20: loss did not improve from 1.75796\n",
            "1128/1128 [==============================] - 16s 15ms/step - loss: 1.7795\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2a61ae718648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the example, a number of weight checkpoint files should be visible in the local directory.\n",
        "\n",
        "All except the one with the smallest loss value can be deleted. The value of the loss is encoded in the filename of the checkpoint to make identifying the best checkpoint simple.\n",
        "\n",
        "If the network loss decreased every epoch, the model would likely benefit from additional training. But before doing that, let's have a look at what this network can generate."
      ],
      "metadata": {
        "id": "Aoc3ErP5o1Cy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Text\n",
        "\n",
        "Generating text using the trained LSTM network is relatively straightforward.\n",
        "\n",
        "First, we load the data and define the network in exactly the same way, except the network weights are loaded from a checkpoint file, and the network does not need to be re-trained. We do this because the last epoch of training may not have had the lowest loss. By reloading the weights form the checkpoint with the lowest loss we ensure that the model is the best it can be.\n",
        "\n",
        "**Note**: You will need to change the name of the checkpoint file in the code below to match the name of the file that has the lowest loss in your run."
      ],
      "metadata": {
        "id": "ZT1QK-0qSCap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the network weights\n",
        "filename = \"weights-improvement-19-1.7580.keras\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "iSbUlDp3SIP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert back from the integer tokens to the unique characters in the text, we create a reverse mapping."
      ],
      "metadata": {
        "id": "mU2ssfTJqVAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "B0b5X_BwSNO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest way to use LSTM model to make predictions is to first start with a seed sequence as input, generate the next character, then update the input to add the generated character on the end and trim off the first character. This process is repeated for as long as desired to predict new characters (e.g., a sequence of 1,000 characters in length).\n",
        "\n",
        "The following defines a function that does exactly that and outputs the generated characters as it goes. It picks a random sequence of characters from the dataset as its initial input pattern, or seed. (We will reuse this function with a larger model below, so the function takes the model to use to generate predictions as a parameter.)"
      ],
      "metadata": {
        "id": "nKsv6d5oqmPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# pick a random seed\n",
        "def generate_text(model, length=1000):\n",
        "  start = np.random.randint(0, len(dataX)-1)\n",
        "  pattern = dataX[start]\n",
        "  output = \"\"\n",
        "  print(\"Seed:\")\n",
        "  print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "  print(\"\\n\\nGenerated:\")\n",
        "  # generate characters\n",
        "  for i in range(length):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    output += result\n",
        "    sys.stdout.write(result)\n",
        "    sys.stdout.flush()\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "  print(\"\\n\\nDone\")"
      ],
      "metadata": {
        "id": "WYQKTgO9SSHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call this function with the model we want it to use to generate the text."
      ],
      "metadata": {
        "id": "tX6O-zT9r_ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model)"
      ],
      "metadata": {
        "id": "lna4uoIkr9aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running this function first outputs the selected random seed, then each character as it is generated.\n",
        "\n",
        "**Note**: Your results will likely vary given the stochastic nature of the algorithm. Consider running the text generation function a few times and compare the average outcome.\n",
        "\n",
        "Some observations about the generated text:\n",
        "\n",
        "- Characters are separated into word-like groups, and some are actual English words, e.g., \"it\", \"to\", \"tea\", \"she\" \"more\", but many are not, e.g., \"hxiniin\", \"lirtle\", \"maae\", \"boeme\"\n",
        "- Occasionally, some of the words in a sequence make sense, e.g., \"and the white rabbit\" but many don't\n",
        "\n",
        "That a small LSTM network learning a character-based model can produce this output is impressive, although far from perfect. In the next section, we will look at improving the quality of the results by developing a larger LSTM network."
      ],
      "metadata": {
        "id": "6z6RawMMrtUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Bigger LSTM Network\n",
        "\n",
        "We got results, but not great results in the previous section. Now, we will try to improve the quality of the generated text by creating a larger network.\n",
        "\n",
        "The definition of the bigger model is not particularly complex, we will simply add a second LSTM layer, with dropout again set to 20% for the second layer. We keep the size of the LSTM layers the same at 256 units."
      ],
      "metadata": {
        "id": "aS-aZauxWyYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigger_model = Sequential()\n",
        "bigger_model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "bigger_model.add(Dropout(0.2))\n",
        "bigger_model.add(LSTM(256, return_sequences=True))\n",
        "bigger_model.add(Dropout(0.2))\n",
        "bigger_model.add(LSTM(256))\n",
        "bigger_model.add(Dropout(0.2))\n",
        "bigger_model.add(Dense(y.shape[1], activation='softmax'))\n",
        "bigger_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "bigger_model.summary()"
      ],
      "metadata": {
        "id": "R64SW96eW3Bg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa73a39-b697-4ece-d34b-954cc62ff289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_7 (LSTM)               (None, 100, 256)          264192    \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 100, 256)          0         \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (None, 100, 256)          525312    \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 100, 256)          0         \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 45)                11565     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,326,381\n",
            "Trainable params: 1,326,381\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll define a different filepath pattern for saving the checkpoints for this bigger model, so that we can tell them apart from the previous checkpoints. We'll also define a new checkpoint callback for this model, so that we aren't competing with the previous lowest loss found before any checkpoints will be saved."
      ],
      "metadata": {
        "id": "StPnK_NvuDDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigger_filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.keras\"\n",
        "bigger_checkpoint = ModelCheckpoint(bigger_filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "bigger_callbacks_list = [bigger_checkpoint]"
      ],
      "metadata": {
        "id": "QIGNl_4YW7Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we increase the number of training epochs from 20 to 50 and decrease the batch size from 128 to 64 to give the network more of an opportunity to be updated and learn. These changes will make the training slower but should result in a much more capable model."
      ],
      "metadata": {
        "id": "ZgCC0vyzujUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "bigger_model.fit(X, y, epochs=50, batch_size=64, callbacks=bigger_callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JZH09vTW7_2",
        "outputId": "b4f20934-cce1-4299-d54c-cb0436e40914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.7942\n",
            "Epoch 1: loss improved from inf to 1.79416, saving model to weights-improvement-01-1.7942-bigger.hdf5\n",
            "2256/2256 [==============================] - 42s 19ms/step - loss: 1.7942\n",
            "Epoch 2/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.7447\n",
            "Epoch 2: loss improved from 1.79416 to 1.74468, saving model to weights-improvement-02-1.7447-bigger.hdf5\n",
            "2256/2256 [==============================] - 42s 19ms/step - loss: 1.7447\n",
            "Epoch 3/50\n",
            "2256/2256 [==============================] - ETA: 0s - loss: 1.7015\n",
            "Epoch 3: loss improved from 1.74468 to 1.70146, saving model to weights-improvement-03-1.7015-bigger.hdf5\n",
            "2256/2256 [==============================] - 44s 19ms/step - loss: 1.7015\n",
            "Epoch 4/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.6629\n",
            "Epoch 4: loss improved from 1.70146 to 1.66295, saving model to weights-improvement-04-1.6630-bigger.hdf5\n",
            "2256/2256 [==============================] - 45s 20ms/step - loss: 1.6630\n",
            "Epoch 5/50\n",
            "2253/2256 [============================>.] - ETA: 0s - loss: 1.6299\n",
            "Epoch 5: loss improved from 1.66295 to 1.62978, saving model to weights-improvement-05-1.6298-bigger.hdf5\n",
            "2256/2256 [==============================] - 42s 19ms/step - loss: 1.6298\n",
            "Epoch 6/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.5990\n",
            "Epoch 6: loss improved from 1.62978 to 1.59905, saving model to weights-improvement-06-1.5990-bigger.hdf5\n",
            "2256/2256 [==============================] - 42s 19ms/step - loss: 1.5990\n",
            "Epoch 7/50\n",
            "2256/2256 [==============================] - ETA: 0s - loss: 1.5689\n",
            "Epoch 7: loss improved from 1.59905 to 1.56894, saving model to weights-improvement-07-1.5689-bigger.hdf5\n",
            "2256/2256 [==============================] - 44s 19ms/step - loss: 1.5689\n",
            "Epoch 8/50\n",
            "2254/2256 [============================>.] - ETA: 0s - loss: 1.5427\n",
            "Epoch 8: loss improved from 1.56894 to 1.54251, saving model to weights-improvement-08-1.5425-bigger.hdf5\n",
            "2256/2256 [==============================] - 43s 19ms/step - loss: 1.5425\n",
            "Epoch 9/50\n",
            "2254/2256 [============================>.] - ETA: 0s - loss: 1.5201\n",
            "Epoch 9: loss improved from 1.54251 to 1.52035, saving model to weights-improvement-09-1.5203-bigger.hdf5\n",
            "2256/2256 [==============================] - 43s 19ms/step - loss: 1.5203\n",
            "Epoch 10/50\n",
            "2256/2256 [==============================] - ETA: 0s - loss: 1.4974\n",
            "Epoch 10: loss improved from 1.52035 to 1.49739, saving model to weights-improvement-10-1.4974-bigger.hdf5\n",
            "2256/2256 [==============================] - 44s 19ms/step - loss: 1.4974\n",
            "Epoch 11/50\n",
            "2256/2256 [==============================] - ETA: 0s - loss: 1.4795\n",
            "Epoch 11: loss improved from 1.49739 to 1.47949, saving model to weights-improvement-11-1.4795-bigger.hdf5\n",
            "2256/2256 [==============================] - 44s 19ms/step - loss: 1.4795\n",
            "Epoch 12/50\n",
            "2254/2256 [============================>.] - ETA: 0s - loss: 1.4545\n",
            "Epoch 12: loss improved from 1.47949 to 1.45456, saving model to weights-improvement-12-1.4546-bigger.hdf5\n",
            "2256/2256 [==============================] - 43s 19ms/step - loss: 1.4546\n",
            "Epoch 13/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.4395\n",
            "Epoch 13: loss improved from 1.45456 to 1.43953, saving model to weights-improvement-13-1.4395-bigger.hdf5\n",
            "2256/2256 [==============================] - 43s 19ms/step - loss: 1.4395\n",
            "Epoch 14/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.4215\n",
            "Epoch 14: loss improved from 1.43953 to 1.42147, saving model to weights-improvement-14-1.4215-bigger.hdf5\n",
            "2256/2256 [==============================] - 43s 19ms/step - loss: 1.4215\n",
            "Epoch 15/50\n",
            "2256/2256 [==============================] - ETA: 0s - loss: 1.4064\n",
            "Epoch 15: loss improved from 1.42147 to 1.40636, saving model to weights-improvement-15-1.4064-bigger.hdf5\n",
            "2256/2256 [==============================] - 44s 19ms/step - loss: 1.4064\n",
            "Epoch 16/50\n",
            "2254/2256 [============================>.] - ETA: 0s - loss: 1.3867\n",
            "Epoch 16: loss improved from 1.40636 to 1.38679, saving model to weights-improvement-16-1.3868-bigger.hdf5\n",
            "2256/2256 [==============================] - 43s 19ms/step - loss: 1.3868\n",
            "Epoch 17/50\n",
            "2256/2256 [==============================] - ETA: 0s - loss: 1.3749\n",
            "Epoch 17: loss improved from 1.38679 to 1.37495, saving model to weights-improvement-17-1.3749-bigger.hdf5\n",
            "2256/2256 [==============================] - 43s 19ms/step - loss: 1.3749\n",
            "Epoch 18/50\n",
            "2256/2256 [==============================] - ETA: 0s - loss: 1.3678\n",
            "Epoch 18: loss improved from 1.37495 to 1.36778, saving model to weights-improvement-18-1.3678-bigger.hdf5\n",
            "2256/2256 [==============================] - 43s 19ms/step - loss: 1.3678\n",
            "Epoch 19/50\n",
            "2256/2256 [==============================] - ETA: 0s - loss: 1.3541\n",
            "Epoch 19: loss improved from 1.36778 to 1.35412, saving model to weights-improvement-19-1.3541-bigger.hdf5\n",
            "2256/2256 [==============================] - 44s 20ms/step - loss: 1.3541\n",
            "Epoch 20/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.3404\n",
            "Epoch 20: loss improved from 1.35412 to 1.34038, saving model to weights-improvement-20-1.3404-bigger.hdf5\n",
            "2256/2256 [==============================] - 44s 20ms/step - loss: 1.3404\n",
            "Epoch 21/50\n",
            "2253/2256 [============================>.] - ETA: 0s - loss: 1.3305\n",
            "Epoch 21: loss improved from 1.34038 to 1.33047, saving model to weights-improvement-21-1.3305-bigger.hdf5\n",
            "2256/2256 [==============================] - 43s 19ms/step - loss: 1.3305\n",
            "Epoch 22/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.3193\n",
            "Epoch 22: loss improved from 1.33047 to 1.31927, saving model to weights-improvement-22-1.3193-bigger.hdf5\n",
            "2256/2256 [==============================] - 44s 19ms/step - loss: 1.3193\n",
            "Epoch 23/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.3092\n",
            "Epoch 23: loss improved from 1.31927 to 1.30917, saving model to weights-improvement-23-1.3092-bigger.hdf5\n",
            "2256/2256 [==============================] - 43s 19ms/step - loss: 1.3092\n",
            "Epoch 24/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.3017\n",
            "Epoch 24: loss improved from 1.30917 to 1.30169, saving model to weights-improvement-24-1.3017-bigger.hdf5\n",
            "2256/2256 [==============================] - 44s 19ms/step - loss: 1.3017\n",
            "Epoch 25/50\n",
            "2256/2256 [==============================] - ETA: 0s - loss: 1.2914\n",
            "Epoch 25: loss improved from 1.30169 to 1.29140, saving model to weights-improvement-25-1.2914-bigger.hdf5\n",
            "2256/2256 [==============================] - 43s 19ms/step - loss: 1.2914\n",
            "Epoch 26/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.2847\n",
            "Epoch 26: loss improved from 1.29140 to 1.28471, saving model to weights-improvement-26-1.2847-bigger.hdf5\n",
            "2256/2256 [==============================] - 44s 20ms/step - loss: 1.2847\n",
            "Epoch 27/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.2772\n",
            "Epoch 27: loss improved from 1.28471 to 1.27727, saving model to weights-improvement-27-1.2773-bigger.hdf5\n",
            "2256/2256 [==============================] - 45s 20ms/step - loss: 1.2773\n",
            "Epoch 28/50\n",
            "2254/2256 [============================>.] - ETA: 0s - loss: 1.2746\n",
            "Epoch 28: loss improved from 1.27727 to 1.27443, saving model to weights-improvement-28-1.2744-bigger.hdf5\n",
            "2256/2256 [==============================] - 45s 20ms/step - loss: 1.2744\n",
            "Epoch 29/50\n",
            "2254/2256 [============================>.] - ETA: 0s - loss: 1.2639\n",
            "Epoch 29: loss improved from 1.27443 to 1.26383, saving model to weights-improvement-29-1.2638-bigger.hdf5\n",
            "2256/2256 [==============================] - 43s 19ms/step - loss: 1.2638\n",
            "Epoch 30/50\n",
            "2256/2256 [==============================] - ETA: 0s - loss: 1.2580\n",
            "Epoch 30: loss improved from 1.26383 to 1.25796, saving model to weights-improvement-30-1.2580-bigger.hdf5\n",
            "2256/2256 [==============================] - 45s 20ms/step - loss: 1.2580\n",
            "Epoch 31/50\n",
            "2254/2256 [============================>.] - ETA: 0s - loss: 1.2522\n",
            "Epoch 31: loss improved from 1.25796 to 1.25213, saving model to weights-improvement-31-1.2521-bigger.hdf5\n",
            "2256/2256 [==============================] - 45s 20ms/step - loss: 1.2521\n",
            "Epoch 32/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.2720\n",
            "Epoch 32: loss did not improve from 1.25213\n",
            "2256/2256 [==============================] - 47s 21ms/step - loss: 1.2720\n",
            "Epoch 33/50\n",
            "2256/2256 [==============================] - ETA: 0s - loss: 1.2613\n",
            "Epoch 33: loss did not improve from 1.25213\n",
            "2256/2256 [==============================] - 46s 21ms/step - loss: 1.2613\n",
            "Epoch 34/50\n",
            "2254/2256 [============================>.] - ETA: 0s - loss: 1.2320\n",
            "Epoch 34: loss improved from 1.25213 to 1.23186, saving model to weights-improvement-34-1.2319-bigger.hdf5\n",
            "2256/2256 [==============================] - 45s 20ms/step - loss: 1.2319\n",
            "Epoch 35/50\n",
            "2254/2256 [============================>.] - ETA: 0s - loss: 1.2321\n",
            "Epoch 35: loss did not improve from 1.23186\n",
            "2256/2256 [==============================] - 43s 19ms/step - loss: 1.2321\n",
            "Epoch 36/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.2278\n",
            "Epoch 36: loss improved from 1.23186 to 1.22778, saving model to weights-improvement-36-1.2278-bigger.hdf5\n",
            "2256/2256 [==============================] - 43s 19ms/step - loss: 1.2278\n",
            "Epoch 37/50\n",
            "2253/2256 [============================>.] - ETA: 0s - loss: 1.2247\n",
            "Epoch 37: loss improved from 1.22778 to 1.22473, saving model to weights-improvement-37-1.2247-bigger.hdf5\n",
            "2256/2256 [==============================] - 44s 20ms/step - loss: 1.2247\n",
            "Epoch 38/50\n",
            "2253/2256 [============================>.] - ETA: 0s - loss: 1.2232\n",
            "Epoch 38: loss improved from 1.22473 to 1.22328, saving model to weights-improvement-38-1.2233-bigger.hdf5\n",
            "2256/2256 [==============================] - 43s 19ms/step - loss: 1.2233\n",
            "Epoch 39/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.2189\n",
            "Epoch 39: loss improved from 1.22328 to 1.21881, saving model to weights-improvement-39-1.2188-bigger.hdf5\n",
            "2256/2256 [==============================] - 44s 19ms/step - loss: 1.2188\n",
            "Epoch 40/50\n",
            "2256/2256 [==============================] - ETA: 0s - loss: 1.2151\n",
            "Epoch 40: loss improved from 1.21881 to 1.21514, saving model to weights-improvement-40-1.2151-bigger.hdf5\n",
            "2256/2256 [==============================] - 45s 20ms/step - loss: 1.2151\n",
            "Epoch 41/50\n",
            "2256/2256 [==============================] - ETA: 0s - loss: 1.2132\n",
            "Epoch 41: loss improved from 1.21514 to 1.21324, saving model to weights-improvement-41-1.2132-bigger.hdf5\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.2132\n",
            "Epoch 42/50\n",
            "2256/2256 [==============================] - ETA: 0s - loss: 1.2020\n",
            "Epoch 42: loss improved from 1.21324 to 1.20202, saving model to weights-improvement-42-1.2020-bigger.hdf5\n",
            "2256/2256 [==============================] - 44s 19ms/step - loss: 1.2020\n",
            "Epoch 43/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.2014\n",
            "Epoch 43: loss improved from 1.20202 to 1.20140, saving model to weights-improvement-43-1.2014-bigger.hdf5\n",
            "2256/2256 [==============================] - 44s 19ms/step - loss: 1.2014\n",
            "Epoch 44/50\n",
            "2254/2256 [============================>.] - ETA: 0s - loss: 1.1987\n",
            "Epoch 44: loss improved from 1.20140 to 1.19870, saving model to weights-improvement-44-1.1987-bigger.hdf5\n",
            "2256/2256 [==============================] - 44s 19ms/step - loss: 1.1987\n",
            "Epoch 45/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.1974\n",
            "Epoch 45: loss improved from 1.19870 to 1.19737, saving model to weights-improvement-45-1.1974-bigger.hdf5\n",
            "2256/2256 [==============================] - 44s 20ms/step - loss: 1.1974\n",
            "Epoch 46/50\n",
            "2254/2256 [============================>.] - ETA: 0s - loss: 1.1966\n",
            "Epoch 46: loss improved from 1.19737 to 1.19660, saving model to weights-improvement-46-1.1966-bigger.hdf5\n",
            "2256/2256 [==============================] - 46s 21ms/step - loss: 1.1966\n",
            "Epoch 47/50\n",
            "2254/2256 [============================>.] - ETA: 0s - loss: 1.1911\n",
            "Epoch 47: loss improved from 1.19660 to 1.19118, saving model to weights-improvement-47-1.1912-bigger.hdf5\n",
            "2256/2256 [==============================] - 45s 20ms/step - loss: 1.1912\n",
            "Epoch 48/50\n",
            "2255/2256 [============================>.] - ETA: 0s - loss: 1.1941\n",
            "Epoch 48: loss did not improve from 1.19118\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.1941\n",
            "Epoch 49/50\n",
            "2256/2256 [==============================] - ETA: 0s - loss: 1.1887\n",
            "Epoch 49: loss improved from 1.19118 to 1.18870, saving model to weights-improvement-49-1.1887-bigger.hdf5\n",
            "2256/2256 [==============================] - 45s 20ms/step - loss: 1.1887\n",
            "Epoch 50/50\n",
            "2254/2256 [============================>.] - ETA: 0s - loss: 1.1860\n",
            "Epoch 50: loss improved from 1.18870 to 1.18601, saving model to weights-improvement-50-1.1860-bigger.hdf5\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.1860\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc00e544650>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running this bigger model, you can expect to achieve a loss of about 1.2. This should be significantly less than the loss achieved for the original model, which would often plateau around 1.6.\n",
        "\n",
        "Before proceeding, make sure that you load the checkpoint for the lowest loss, in case this wasn't the final epoch."
      ],
      "metadata": {
        "id": "C0pN4hiDuusp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the network weights\n",
        "bigger_filename = \"weights-improvement-20-1.1860-bigger.keras\"\n",
        "bigger_model.load_weights(bigger_filename)\n",
        "bigger_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "RAaxNhM2vP_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now run the function that we defined earlier with the bigger model, to generate some text."
      ],
      "metadata": {
        "id": "I5UWdV4gvn-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(bigger_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMSh6zyeksKY",
        "outputId": "73cac515-759d-45bf-e178-b99180689e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\"  alice\n",
            "for some time with great curiosity, and this was his first speech.\n",
            "\n",
            "'you should learn not to  \"\n",
            "\n",
            "\n",
            "Generated:\n",
            "the birtant ' said the queen, \n",
            "'i don't know what you were it in a parrer as it was,' the mock turtle seplied in an offended tone. and the queen surnid another vith one of the gall.\n",
            "and the queen she heard the queen was she was a little shriek sile she was a git drrter, and the queen she heard the queen was sie was a little dourte, and the queen surnid nut the rabbit hole of the dorrs, and the pueen said to the gryphon. \n",
            "'what do you think you miked to see it tr and sereamed to be a very dire, i should think you were the door way i manegeng ane more of the cance.\n",
            "\n",
            " will you, won't you, will you, will you, will you, will you, will you, will you, will you, will you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't you, won't yo\n",
            "\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are generally fewer spelling mistakes, and the text looks more realistic but is still quite nonsensical.\n",
        "\n",
        "For example, the same phrases get repeated again and again, e.g., \"will you\" and \"won't you\". Quotes are opened but not closed.\n",
        "\n",
        "The results of this model are significantly better than the previous model, but there is still a lot of room for improvement."
      ],
      "metadata": {
        "id": "27wFNF-0vvWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving the Model\n",
        "\n",
        "Here are some ideas for improving the model that you might want to try:\n",
        "\n",
        "- Predict fewer than 1,000 characters as output for a given seed (the LSTM performs better the closer it is to the seed, so shorter sequences should be more coherent)\n",
        "- Clean the source text more thoroughly, e.g., remove all punctuation from the source text and, therefore, from the models’ vocabulary\n",
        "- Try a different source text; Alice in Wonderland is a brilliant book but Lewis Carroll was a master of non-sensical rhymes and it may be easier to judge the success of the model on a different source\n",
        "- Increase the number of training epochs, you can easily do this by loading a checkpoint and continuing the training from there\n",
        "- Try changing the dropout percentage to see if this has a noticeable impact on the generated output\n",
        "- Try changing the batch size, e.g., start with a batch size of 1 and slowly increase the batch size to see if you can find one that performs better\n",
        "- Add more memory units and/or layers to the model\n",
        "\n",
        "## Tutorial Assignment\n",
        "\n",
        "Attempt at least 2 of the above suggestions (or some changes of your own devising, e.g., you might want to replace the LSTM layers with GRU layers and see how this impacts speed of training and performance on your dataset). Report on your experiments by submitting your notebook with comments on the things that you tried, what worked better than expected, what worked worse."
      ],
      "metadata": {
        "id": "RW2FswKOwUJw"
      }
    }
  ]
}